import os
import re
from dotenv import load_dotenv
from pypdf import PdfReader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

load_dotenv()

def load_manual_pdf():
    """ë§¤ë‰´ì–¼ PDF ë¡œë“œ"""
    print("\n[STEP 1] ğŸ“‚ PDF ë¡œë”© ì¤‘...")
    
    pdf_path = "data/manual.pdf"
    reader = PdfReader(pdf_path)
    
    print(f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(reader.pages)}í˜ì´ì§€")
    
    return reader

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # 1. ìˆ«ìë§Œ ìˆëŠ” ì¤„ ì œê±° (í˜ì´ì§€ ë²ˆí˜¸)
        if line.strip().isdigit():
            continue
        
        # 2. ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±° (5ì ë¯¸ë§Œ)
        if len(line.strip()) < 5:
            continue
        
        # 3. íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ì¤„ ì œê±°
        if re.match(r'^[\s\W]+$', line):
            continue
        
        cleaned_lines.append(line.strip())
    
    # 4. í…ìŠ¤íŠ¸ ì¬ì¡°í•©
    cleaned_text = '\n'.join(cleaned_lines)
    
    # 5. ë‹¤ì¤‘ ê°œí–‰ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    
    # 6. ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    cleaned_text = re.sub(r' {2,}', ' ', cleaned_text)
    
    return cleaned_text

def extract_core_pages(reader):
    """í•µì‹¬ í˜ì´ì§€ ì¶”ì¶œ + ì „ì²˜ë¦¬"""
    print("\n[STEP 2] ğŸ” í•µì‹¬ í˜ì´ì§€ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì¤‘...")
    
    # í•µì‹¬ í˜ì´ì§€ (0-based index)
    core_pages = [
        10, 11, 12, 13,  # ìì‚´ ì§•í›„ (p.11-14)
        17, 18, 19,      # ëŒ€ë©´ ë©´ë‹´ ì˜ˆì‹œ (p.18-20)
        23, 24, 25,      # ìœ„ê¸° ê°œì… (p.24-26)
        26, 27, 28, 29   # ìœ„í—˜ìš”ì¸/ë³´í˜¸ìš”ì¸ (p.27-30)
    ]
    
    documents = []
    
    for page_num in core_pages:
        try:
            page = reader.pages[page_num]
            text = page.extract_text()
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = clean_text(text)
            
            # ìµœì†Œ ê¸¸ì´ ì²´í¬ (100ì ë¯¸ë§Œì€ ì œì™¸)
            if len(cleaned_text) < 100:
                print(f"âš ï¸  í˜ì´ì§€ {page_num + 1}: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(cleaned_text)}ì) - ê±´ë„ˆëœ€")
                continue
            
            doc = Document(
                page_content=cleaned_text,
                metadata={
                    "page": page_num + 1,  # 1-based for display
                    "source": "í•™ìƒìì‚´ìœ„ê¸°ëŒ€ì‘ë§¤ë‰´ì–¼"
                }
            )
            documents.append(doc)
            
            print(f"âœ… í˜ì´ì§€ {page_num + 1}: {len(cleaned_text)}ì ì¶”ì¶œ")
            
        except Exception as e:
            print(f"âš ï¸  í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    print(f"\nâœ… {len(documents)}ê°œ í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
    
    return documents

def chunk_documents(documents):
    """ë¬¸ì„œ ì²­í‚¹"""
    print("\n[STEP 3] âœ‚ï¸  ì²­í‚¹ ì¤‘...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,       # 1000 â†’ 1500 (ì¦ê°€)
        chunk_overlap=200,     # 150 â†’ 200 (ì¦ê°€)
        separators=[
            "\n\n",  # ë‹¨ë½ êµ¬ë¶„
            "\n",    # ì¤„ë°”ê¿ˆ
            "ã€‚",    # í•œêµ­ì–´ ë§ˆì¹¨í‘œ
            ".",     # ì˜ì–´ ë§ˆì¹¨í‘œ
            "!",     # ëŠë‚Œí‘œ
            "?",     # ë¬¼ìŒí‘œ
            " ",     # ê³µë°±
            ""       # ë¬¸ì
        ]
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    
    # ì²­í¬ ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 3ê°œ)
    if chunks:
        print(f"\n=== ì²­í¬ ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 3ê°œ) ===")
        for i, chunk in enumerate(chunks[:3], 1):
            print(f"\n[ì²­í¬ {i}]")
            print(f"  í˜ì´ì§€: {chunk.metadata.get('page', 'N/A')}")
            print(f"  ê¸¸ì´: {len(chunk.page_content)}ì")
            print(f"  ë‚´ìš©: {chunk.page_content[:150]}...")
    
    return chunks

def embed_to_pinecone(chunks):
    """Pineconeì— ì„ë² ë”©"""
    print("\n[STEP 4] ğŸš€ Pineconeì— ì„ë² ë”© ì¤‘...")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    if not os.getenv("PINECONE_API_KEY"):
        print("âŒ PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì„ë² ë”© ëª¨ë¸
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    
    # Pinecone ì¸ë±ìŠ¤ í™•ì¸
    index_name = 'student-counseling-manual'
    
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    existing_indexes = [index.name for index in pc.list_indexes()]
    
    if index_name not in existing_indexes:
        print(f"\nâŒ ì¸ë±ìŠ¤ '{index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("\nğŸ“ Pinecone ì½˜ì†”ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:")
        print(f"   - Index Name: {index_name}")
        print(f"   - Dimensions: 3072 (text-embedding-3-large)")
        print(f"   - Metric: cosine")
        print(f"   - Cloud: AWS")
        print(f"   - Region: us-east-1")
        return None
    
    print(f"âœ… ì¸ë±ìŠ¤ í™•ì¸: {index_name}")
    
    # ì„ë² ë”© & ì—…ë¡œë“œ
    print(f"â³ ì„ë² ë”© ì¤‘... (ì•½ 10-20ì´ˆ ì†Œìš”)")
    
    vectorstore = PineconeVectorStore.from_documents(
        documents=chunks,
        embedding=embedding,
        index_name=index_name
    )
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ!")
    
    return vectorstore

def verify_embeddings():
    """ì„ë² ë”© ê²€ì¦"""
    print("\n[STEP 5] ğŸ” ì„ë² ë”© ê²€ì¦ ì¤‘...")
    
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    index_name = 'student-counseling-manual'
    
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ìì‚´ ì§•í›„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "í•™ìƒê³¼ ëŒ€ë©´ ë©´ë‹´ ì‹œ ì£¼ì˜ì‚¬í•­",
        "ë¶€ëª¨ë‹˜ê»˜ ì–´ë–»ê²Œ ì „ë‹¬í•˜ë‚˜ìš”?"
    ]
    
    print("\n=== ê²€ì¦ í…ŒìŠ¤íŠ¸ ===")
    for query in test_queries:
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: '{query}'")
        results = vectorstore.similarity_search(query, k=2)
        
        if results:
            print(f"âœ… {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            print(f"   í˜ì´ì§€: {results[0].metadata.get('page', 'N/A')}")
            print(f"   ê¸¸ì´: {len(results[0].page_content)}ì")
            print(f"   ë‚´ìš©: {results[0].page_content[:200]}...")
        else:
            print("âŒ ê²°ê³¼ ì—†ìŒ")
    
    print("\nâœ… ê²€ì¦ ì™„ë£Œ!")

def main():
    print("=" * 80)
    print("ğŸ“ í•™ìƒ ìì‚´ìœ„ê¸° ëŒ€ì‘ ë§¤ë‰´ì–¼ ì„ë² ë”© (ìµœì¢…ë³¸)")
    print("=" * 80)
    
    try:
        # 1. PDF ë¡œë“œ
        reader = load_manual_pdf()
        
        # 2. í•µì‹¬ í˜ì´ì§€ ì¶”ì¶œ + ì „ì²˜ë¦¬
        documents = extract_core_pages(reader)
        
        if not documents:
            print("\nâŒ ì¶”ì¶œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ì²­í‚¹
        chunks = chunk_documents(documents)
        
        # 4. Pinecone ì„ë² ë”©
        vectorstore = embed_to_pinecone(chunks)
        
        if vectorstore:
            # 5. ê²€ì¦
            verify_embeddings()
            
            print("\n" + "=" * 80)
            print("ğŸ‰ ì™„ë£Œ! ì´ì œ ì±—ë´‡ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
            print("   1. ê²€ì¦ ê²°ê³¼ í™•ì¸")
            print("   2. Pinecone ì½˜ì†”ì—ì„œ ë°ì´í„° í™•ì¸")
            print("   3. streamlit run app.py")
            print("=" * 80)
        
    except FileNotFoundError:
        print("\nâŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        print("   data/manual.pdf")
        print("\nğŸ’¡ íŒŒì¼ì„ data/ í´ë”ì— ë„£ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()