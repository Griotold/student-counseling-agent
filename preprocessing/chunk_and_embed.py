"""
ì²­í‚¹ ë° ì„ë² ë”©
data/all_pages_txt/ ì˜ txt íŒŒì¼ë“¤ì„ ì²­í‚¹í•˜ê³  Pineconeì— ì„ë² ë”©
"""
import os
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def load_all_texts():
    """ëª¨ë“  txt íŒŒì¼ ë¡œë“œ"""
    print("=" * 80)
    print("ğŸ“‚ txt íŒŒì¼ ë¡œë“œ")
    print("=" * 80)
    
    txt_dir = Path("data/all_pages_txt")
    txt_files = sorted(txt_dir.glob("page_*.txt"))
    
    documents = []
    
    for txt_file in txt_files:
        with open(txt_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # ë©”íƒ€ë°ì´í„° í¬í•¨
        page_num = txt_file.stem.split('_')[1]
        
        documents.append({
            'text': text,
            'metadata': {
                'source': str(txt_file),
                'page': int(page_num)
            }
        })
        
        print(f"âœ… {txt_file.name}: {len(text)}ì")
    
    print(f"\nì´ {len(documents)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    total_chars = sum(len(doc['text']) for doc in documents)
    print(f"ì´ ê¸€ììˆ˜: {total_chars:,}ì")
    
    return documents

def chunk_documents(documents):
    """ë¬¸ì„œ ì²­í‚¹"""
    print("\n" + "=" * 80)
    print("âœ‚ï¸  í…ìŠ¤íŠ¸ ì²­í‚¹")
    print("=" * 80)
    
    # ì²­í‚¹ ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,        # ì²­í¬ í¬ê¸°
        chunk_overlap=200,      # ì¤‘ë³µ í¬ê¸°
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    
    all_chunks = []
    
    for doc in documents:
        # ì²­í‚¹
        chunks = text_splitter.split_text(doc['text'])
        
        # ë©”íƒ€ë°ì´í„° í¬í•¨
        for i, chunk in enumerate(chunks):
            all_chunks.append({
                'text': chunk,
                'metadata': {
                    **doc['metadata'],
                    'chunk_index': i,
                    'total_chunks': len(chunks)
                }
            })
        
        print(f"í˜ì´ì§€ {doc['metadata']['page']}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    print(f"\nì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # ì²­í¬ í¬ê¸° ë¶„í¬ í™•ì¸
    chunk_sizes = [len(chunk['text']) for chunk in all_chunks]
    avg_size = sum(chunk_sizes) / len(chunk_sizes)
    min_size = min(chunk_sizes)
    max_size = max(chunk_sizes)
    
    print(f"\nì²­í¬ í¬ê¸°:")
    print(f"  í‰ê· : {avg_size:.0f}ì")
    print(f"  ìµœì†Œ: {min_size}ì")
    print(f"  ìµœëŒ€: {max_size}ì")
    
    return all_chunks

def embed_and_store(chunks):
    """ì„ë² ë”© ë° Pinecone ì €ì¥"""
    print("\n" + "=" * 80)
    print("ğŸ”¢ ì„ë² ë”© ë° Pinecone ì €ì¥")
    print("=" * 80)
    
    # OpenAI Embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        dimensions=3072
    )
    
    # Pinecone ì´ˆê¸°í™”
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index_name = "student-counseling-0202"
    
    print(f"\nì¸ë±ìŠ¤: {index_name}")
    print(f"ì„ë² ë”© ëª¨ë¸: text-embedding-3-large (3072ì°¨ì›)")
    print(f"ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    from langchain.schema import Document
    
    docs = [
        Document(
            page_content=chunk['text'],
            metadata=chunk['metadata']
        )
        for chunk in chunks
    ]
    
    # Pineconeì— ì €ì¥
    print("\nì„ë² ë”© ì¤‘... (ì•½ 30ì´ˆ-1ë¶„ ì†Œìš”)")
    
    vectorstore = PineconeVectorStore.from_documents(
        documents=docs,
        embedding=embeddings,
        index_name=index_name
    )
    
    print("âœ… Pinecone ì €ì¥ ì™„ë£Œ!")
    
    return vectorstore

def verify_pinecone():
    """Pinecone ì €ì¥ í™•ì¸"""
    print("\n" + "=" * 80)
    print("âœ“ Pinecone ì €ì¥ í™•ì¸")
    print("=" * 80)
    
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index = pc.Index("student-counseling-0202")
    
    stats = index.describe_index_stats()
    
    print(f"\nì¸ë±ìŠ¤ í†µê³„:")
    print(f"  ì´ ë²¡í„° ìˆ˜: {stats['total_vector_count']}")
    print(f"  ì°¨ì›: {stats['dimension']}")
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n" + "=" * 80)
    print("ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("=" * 80)
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        dimensions=3072
    )
    
    vectorstore = PineconeVectorStore(
        index_name="student-counseling-0202",
        embedding=embeddings
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ìì‚´ ì§•í›„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "í•™ìƒì´ ì£½ê³  ì‹¶ë‹¤ê³  ë§í•˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ë¶€ëª¨ì—ê²Œ ì–´ë–»ê²Œ ì•Œë ¤ì•¼ í•˜ë‚˜ìš”?"
    ]
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        results = vectorstore.similarity_search(query, k=2)
        
        for i, doc in enumerate(results, 1):
            print(f"\n  ê²°ê³¼ {i}:")
            print(f"    í˜ì´ì§€: {doc.metadata.get('page')}")
            print(f"    ë‚´ìš©: {doc.page_content[:100]}...")

def main():
    print("=" * 80)
    print("ğŸš€ ì²­í‚¹ ë° ì„ë² ë”© íŒŒì´í”„ë¼ì¸")
    print("=" * 80)
    
    # 1. txt íŒŒì¼ ë¡œë“œ
    documents = load_all_texts()
    
    # # 2. ì²­í‚¹
    chunks = chunk_documents(documents)
    
    # # 3. ì„ë² ë”© ë° ì €ì¥
    vectorstore = embed_and_store(chunks)
    
    # 4. í™•ì¸
    verify_pinecone()
    
    print("\n" + "=" * 80)
    print("âœ¨ ì™„ë£Œ!")
    print("=" * 80)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. Agent ì‹¤í–‰")
    print("  2. í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()